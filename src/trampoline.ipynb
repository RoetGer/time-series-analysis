{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.5.2"},"colab":{"name":"trampoline.ipynb","provenance":[{"file_id":"https://github.com/mlcollege/time-series-analysis/blob/master/src/trampoline.ipynb","timestamp":1575041088833}],"collapsed_sections":[]}},"cells":[{"cell_type":"markdown","metadata":{"id":"CmbVMF8nvmzO","colab_type":"text"},"source":["# Time series - binary classification\n","The following example shows binary classification task on time series data. There are several approaches possible. We will try two of them:\n","\n","- **Feedforward neural network** fixed window training & prediction\n","- **Recurrent neural network** fixed window training & continuous prediction\n","\n","The task is to classify (detect) a jump performed by a trampoline jumper during her training. We would classify a specific jump (called twist - backflip with one spin) from the other jumps.\n","\n","First, we need some imports. We won't use `pandas` for this example, just `numpy`:"]},{"cell_type":"code","metadata":{"id":"mrbzRWCcvmzT","colab_type":"code","colab":{}},"source":["%tensorflow_version 2.x\n","%matplotlib inline\n","\n","\n","import numpy as np\n","import os\n","import matplotlib.pyplot as plt"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pHTrrQUuvmzY","colab_type":"text"},"source":["## Dataset preparation\n","Dataset consist of several hundreds of timeseries. It's separated into two folders, the first contains *positive samples* - twists, and the other contains *negative samples* - other jumps.\n","\n","Eeach jump is captured in a CSV file that contains 13 values per time step."]},{"cell_type":"code","metadata":{"id":"HcxJrWbkvmza","colab_type":"code","colab":{}},"source":["def read_csv_files(path):\n","    dirpath, dirnames, filenames = list(os.walk(path))[0]    \n","    return [\n","        np.genfromtxt(dirpath + '/' + file, delimiter=',') for file in filenames \n","        if os.path.splitext(file)[1] == '.csv'\n","    ]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"2o0NrKW1vvIv","colab_type":"code","colab":{}},"source":["from google.colab import drive\n","drive.mount('/content/drive/')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Wh4MCbu8vmze","colab_type":"code","colab":{}},"source":["import zipfile\n","\n","ZIPPED_DATA = '/content/drive/My Drive/ml-college/time-series-analysis/data/trampoliny.zip'\n","DATA_FOLDER = '/content'\n","\n","with zipfile.ZipFile(ZIPPED_DATA, 'r') as zip_ref:\n","    zip_ref.extractall(DATA_FOLDER)\n","\n","positive_samples = read_csv_files(DATA_FOLDER + '/trampoliny/42')\n","negative_samples = read_csv_files(DATA_FOLDER + '/trampoliny/ostatni')"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RU_D6RWovmzh","colab_type":"text"},"source":["There are several hundreads of samples in the datasets. Their lengths vary from ~20 to ~120 timesteps."]},{"cell_type":"code","metadata":{"id":"g38S3CSrvmzi","colab_type":"code","colab":{}},"source":["print(\"Positive samples: %d\" % len(positive_samples))\n","print(\"Negative samples: %d\" % len(negative_samples))\n","\n","#TODO: add correct collections of lengths of samples to see histograms of lengths\n","\n","plt.hist(...)\n","plt.hist(...)\n","plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xLebuhY2vmzm","colab_type":"text"},"source":["### Columns selection\n","Data were captured using a motion sensor attached to the jumper's leg during her training. The motion sensor can give us several quantities:\n","\n","- linear acceleration (3D vector)\n","- angual acceleration (3D vector)\n","- direction of gravity (3D sensor)\n","- orientation (4D quaternion)\n","\n","Quantities are captured in different frequencies and all dataset was resampled to the the same frequency using nearies neighbour resampling (see \"steps\" in the plotted curves)."]},{"cell_type":"code","metadata":{"scrolled":false,"id":"PEOOdDF5vmzn","colab_type":"code","colab":{}},"source":["plt.figure(figsize=(15,6))\n","plt.plot(positive_samples[0])\n","plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"m9veuyhWvmzq","colab_type":"text"},"source":["Orientation values seems to vary a lot, let's ignore them for the training. (We can keep them there but the model would probably ignore them anyway)"]},{"cell_type":"code","metadata":{"id":"vXac3AJHvmzr","colab_type":"code","colab":{}},"source":["plt.figure(figsize=(15,6))\n","plt.plot(positive_samples[0][:,9:13])\n","plt.show()\n","\n","plt.figure(figsize=(15,6))\n","plt.plot(positive_samples[0][:,0:9])\n","plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"09ggrC47vmzu","colab_type":"text"},"source":["### Normalization & padding\n","The values returned the sensor is given by it's digital nature (usually between -32k, 32k) and needs to be normalized (column-wise)."]},{"cell_type":"code","metadata":{"id":"iHMb2gMMvmzv","colab_type":"code","colab":{}},"source":["def normalize(*datasets):            \n","    all_samples = np.vstack([np.vstack(samples) for samples in datasets])    \n","    #TODO: add `min_vals` and `max_vals` arrays here, both should have shape (13,)\n","    \n","    return [\n","        #TODO construct a collection where XXX is a normalized sample:  [XXX for sample in samples], normalize to <-1, 1>\n","        for samples in datasets\n","    ], min_vals, max_vals"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"clUhp5-Fvmzy","colab_type":"code","colab":{}},"source":["(norm_positive_samples, norm_negative_samples), max_vals, min_vals = normalize(positive_samples, negative_samples)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4bvChx6Pvmz1","colab_type":"text"},"source":["Also, for tensor-based training, the data need to be padded to a fixed length. Let's take the maximum length and pad all sequences with zeros (leading)."]},{"cell_type":"code","metadata":{"id":"P4ka9hz2vmz2","colab_type":"code","colab":{}},"source":["from keras.preprocessing.sequence import pad_sequences\n","\n","def pad(*datasets):\n","    max_length = #TODO: get a maximal length of a sample here\n","    return [pad_sequences(samples, maxlen=max_length, dtype=datasets[0][0].dtype) for samples in datasets]"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Dtwybv9_vmz5","colab_type":"text"},"source":["..now, let's see how our samples look like after normalization and padding"]},{"cell_type":"code","metadata":{"id":"SOzG-Ptpvmz6","colab_type":"code","colab":{}},"source":["norm_positive_samples, norm_negative_samples = pad(norm_positive_samples, norm_negative_samples)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"_rIqmIWEvmz9","colab_type":"code","colab":{}},"source":["plt.figure(figsize=(15,6))\n","plt.plot(norm_positive_samples[0][:,0:9])\n","plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9nG61BAdvm0A","colab_type":"text"},"source":["## Model training\n","Now, we are ready to train the model. Let's start with training set construction\n","\n","Notice how we are creating the target variable by filling correct number of *zeroes* and *ones* into the `training_Y` array."]},{"cell_type":"code","metadata":{"id":"-OZljyL4vm0A","colab_type":"code","colab":{}},"source":["training_X = np.vstack((norm_positive_samples[:,:,0:9], norm_negative_samples[:,:,0:9]))\n","training_Y = #TODO: generate correct labels for our input series"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SCT7SpHzvm0D","colab_type":"text"},"source":["We want to use `validation_split` and we need to shuffle the training set randomly. It needs to be performed \"pair-wise\"."]},{"cell_type":"code","metadata":{"id":"RmtcJciCvm0E","colab_type":"code","colab":{}},"source":["import random\n","\n","#TODO: shuffle the training set !element wise!\n","\n","training_X =\n","training_Y = "],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9xC-U6Xqvm0H","colab_type":"text"},"source":["### Feed-forward model\n","Let's start with the simple **feed-forward network** with no recurrent connections. \n","\n","In this case, we need to reshape out input data as we are feeding it all together in one step (127x9 values)"]},{"cell_type":"code","metadata":{"id":"PCrMCMfmvm0I","colab_type":"code","colab":{}},"source":["from keras import Model\n","from keras.layers import LSTM, Input, Dense\n","\n","inputs = Input(shape=(training_X.shape[1] * training_X.shape[2],))\n","x = \n","#TODO add three dense layers with tanh activation (256,128,64)\n","\n","outputs = Dense(1, activation='sigmoid')(x)\n","\n","ffn_model = Model(inputs, outputs)\n","ffn_model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['accuracy'])\n","ffn_model.summary()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"nqwxYjLkvm0K","colab_type":"code","colab":{}},"source":["#TODO: why we need this? replace ... with correct window size\n","ffn_training_X = training_X.reshape(training_X.shape[0], ... )"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"scrolled":false,"id":"_Z00ZXv3vm0N","colab_type":"code","colab":{}},"source":["#TODO run model.fit on training data with validation split 0.1 and 40 epochs"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3aYGTVQbvm0Q","colab_type":"text"},"source":["We finished with ~90% accuracy on the validation set.\n","\n","We can do better with recurrent nets. Both better accuracy and much smaller model.\n","\n","### LSTM recurrent model\n","\n","Let's take famous LSTM units and make smaller 2-layered network out of them. \n","\n","Notice the shape of the input. LSTMs are recurrent networds and they expects sequences of inputs):"]},{"cell_type":"code","metadata":{"id":"yo4k9SVavm0R","colab_type":"code","colab":{}},"source":["inputs = Input(shape=training_X.shape[1:])\n","\n","#TODO: create and compile model of two layers of LSTMs with one Dense output at the end of the sequence. Fit function must work with it."],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"scrolled":false,"id":"sDWlw1Bavm0V","colab_type":"code","colab":{}},"source":["model.fit(training_X, training_Y, epochs=30, validation_split=0.1)\n","model.save_weights('model_trampoline_9i.hdf')"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"cDYxpGcIvm0Z","colab_type":"text"},"source":["We finished with >90% accuracy on the validation set.\n","### Predicion phase\n","\n","Now, let's build the prediction model. We will use the same architecture (LSTMs) but now we are aiming for continuous prediction (return output value for each timestep)."]},{"cell_type":"code","metadata":{"id":"v-tnzDXavm0Z","colab_type":"code","colab":{}},"source":["from keras import Model\n","from keras.layers import LSTM, Input, Dense, Dropout\n","\n","inputs = Input(shape=(None, training_X.shape[2]))\n","#TODO: repeat the model layers but return an output at every time step\n","outputs = Dense(1, activation='sigmoid')(x)\n","\n","cont_model = Model(inputs, outputs)\n","cont_model.summary()\n","cont_model.load_weights(\"model_trampoline_9i.hdf\")"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4QtZzEFYvm0e","colab_type":"text"},"source":["Let's see how the network works throughout the whole sequence:"]},{"cell_type":"code","metadata":{"id":"Elx0NNPDvm0f","colab_type":"code","colab":{}},"source":["def show_prediction(test_case):\n","    print(test_case[1])\n","\n","    c_prediction = #TODO: call model predict on test_case\n","    plt.figure(figsize=(15,6))\n","    plt.plot(test_case[0], 'silver')\n","    plt.plot(c_prediction[0], 'red' if test_case[1] == 0 else 'green')\n","    plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"iWwk7FNQvm0h","colab_type":"code","colab":{}},"source":["positive_test_sample = next(sample for sample in reversed(training_set) if sample[1] == 1)\n","negative_test_sample = next(sample for sample in reversed(training_set) if sample[1] == 0)\n","\n","show_prediction(positive_test_sample)\n","show_prediction(negative_test_sample)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"m7kU78oNvm0k","colab_type":"text"},"source":["The LSTM prediction model is not restricted to a fixed sequence length and can predict for arbitrary sequence length:"]},{"cell_type":"code","metadata":{"id":"H4PBnZesvm0l","colab_type":"code","colab":{}},"source":["#TODO: make LSTM model predict from truncated test sample"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nDcgeSWFvm0o","colab_type":"text"},"source":["## Analysis\n","\n","Let's make some analysis on the whole dataset and finish our experiment with standard performance measures.\n","\n","First, let's see how our model perform on various sequence lengths:"]},{"cell_type":"code","metadata":{"id":"JXEs7f-1vm0p","colab_type":"code","colab":{}},"source":["plt.figure(figsize=(25,6))\n","c_prediction = cont_model.predict(norm_positive_samples[:100,:,0:9])\n","plt.plot(np.squeeze(c_prediction).T, '#00800020')\n","c_prediction = cont_model.predict(norm_negative_samples[:100,:,0:9])\n","plt.plot(np.squeeze(c_prediction).T, '#80000020')\n","plt.show()\n","\n","plt.figure(figsize=(25,6))\n","c_prediction = cont_model.predict(norm_positive_samples[:100,60:,0:9])\n","plt.plot(np.squeeze(c_prediction).T, '#00800020')\n","c_prediction = cont_model.predict(norm_negative_samples[:100,60:,0:9])\n","plt.plot(np.squeeze(c_prediction).T, '#80000020')\n","plt.show()\n","\n","plt.figure(figsize=(25,6))\n","c_prediction = cont_model.predict(norm_positive_samples[:100,100:,0:9])\n","plt.plot(np.squeeze(c_prediction).T, '#00800020')\n","c_prediction = cont_model.predict(norm_negative_samples[:100,100:,0:9])\n","plt.plot(np.squeeze(c_prediction).T, '#80000020')\n","plt.show()\n","\n","plt.figure(figsize=(25,6))\n","c_prediction = cont_model.predict(norm_positive_samples[:100,117:,0:9])\n","plt.plot(np.squeeze(c_prediction).T, '#00800020')\n","c_prediction = cont_model.predict(norm_negative_samples[:100,117:,0:9])\n","plt.plot(np.squeeze(c_prediction).T, '#80000020')\n","plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_ItkUSAUvm0r","colab_type":"text"},"source":["Finally let's see what would be the optimal threshold for application of our model:"]},{"cell_type":"code","metadata":{"id":"BOPe0ZRovm0s","colab_type":"code","colab":{}},"source":["from sklearn.metrics import roc_curve, precision_recall_curve\n","\n","threshold = 0.2\n","\n","fpr, tpr, thresholds = roc_curve(...) #TODO: Search the documentation and add correct params here\n","\n","fig = plt.figure(figsize=(10, 8))\n","plt.plot([0, 1], [0, 1], 'k--')\n","plt.plot(fpr, tpr)\n","\n","t_index = min(enumerate(thresholds), key=lambda x: abs(x[1] - threshold))[0]\n","s = plt.scatter(fpr[t_index], tpr[t_index])\n","s.axes.annotate(thresholds[t_index], (fpr[t_index] + 0.01, tpr[t_index] - 0.02))\n","\n","    \n","plt.xlabel('False positive rate')\n","plt.ylabel('True positive rate')\n","plt.show()\n","\n","precision, recall, thresholds = precision_recall_curve(...) #TODO: Search the documentation and add correct params here\n","\n","fig = plt.figure(figsize=(10, 8))\n","plt.plot(recall, precision)\n","\n","t_index = min(enumerate(thresholds), key=lambda x: abs(x[1] - threshold))[0]\n","s = plt.scatter(recall[t_index], precision[t_index])\n","s.axes.annotate(thresholds[t_index], (recall[t_index] + 0.01, precision[t_index] + 0.002))\n","\n","plt.xlabel('Recall')    \n","plt.ylabel('Precision')\n","plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"CVpnc7Onvm0u","colab_type":"code","colab":{}},"source":["from sklearn.metrics import confusion_matrix\n","\n","confusion_matrix(true_Y, prediction)"],"execution_count":0,"outputs":[]}]}